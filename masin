from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer, mean_poisson_deviance
import xgboost as xgb
from typing import List, Dict, Tuple, Any, Optional
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import warnings
from datetime import datetime

try:
    # scikit-optimize Gaussian Process Bayesian optimization - REQUIRED dependency
    from skopt import gp_minimize                                 # ← GP Bayesian optimization
    from skopt.space import Real, Integer                         # ← skopt parameter spaces  
    from skopt.utils import use_named_args                        # ← skopt utilities
except ImportError as e:
    raise ImportError(
        "scikit-optimize is required for this frequency model. "
        "Install with: pip install scikit-optimize"
    ) from e

warnings.filterwarnings('ignore')


def neg_poisson_deviance_scorer(y_true, y_pred):
    """Sklearn scorer for negative Poisson deviance (higher is better)
    Uses sklearn's built-in mean_poisson_deviance for better reliability and consistency"""
    return -mean_poisson_deviance(y_true, y_pred)


# Create sklearn scorer using built-in mean_poisson_deviance
# sklearn's implementation is more robust and handles edge cases better
try:
    # Try to use sklearn's built-in scorer string first
    from sklearn.metrics import get_scorer
    poisson_scorer = get_scorer('neg_mean_poisson_deviance')
    print("Using sklearn's built-in 'neg_mean_poisson_deviance' scorer")
except (ValueError, KeyError):
    # Fall back to custom scorer if built-in doesn't exist
    poisson_scorer = make_scorer(neg_poisson_deviance_scorer, greater_is_better=True)
    print("Using custom negative Poisson deviance scorer")


class FeatureFilter(BaseEstimator, TransformerMixin):
    """Custom sklearn transformer for feature filtering"""
    
    def __init__(self, min_sample_ratio=0.01, min_samples=50):
        self.min_sample_ratio = min_sample_ratio
        self.min_samples = min_samples
        self.features_to_keep_ = None
    
    def fit(self, X, y=None):
        """Learn which features to keep based on minimum samples requirement"""
        X_df = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X
        
        min_required = max(self.min_samples, len(X_df) * self.min_sample_ratio)
        self.features_to_keep_ = X_df.apply(lambda x: x.gt(0).sum() >= min_required, axis=0)
        
        filtered_count = len(X_df.columns) - self.features_to_keep_.sum()
        if filtered_count > 0:
            print(f"FeatureFilter: Will filter out {filtered_count} noisy features")
            
        return self
    
    def transform(self, X):
        """Apply feature filtering"""
        if self.features_to_keep_ is None:
            raise ValueError("FeatureFilter not fitted yet")
            
        X_df = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X
        return X_df.loc[:, self.features_to_keep_].values
    
    def get_feature_names_out(self, input_features=None):
        """Get output feature names"""
        if self.features_to_keep_ is None:
            raise ValueError("FeatureFilter not fitted yet")
            
        if input_features is None:
            return [f"feature_{i}" for i in range(self.features_to_keep_.sum())]
        else:
            return [name for name, keep in zip(input_features, self.features_to_keep_) if keep]


class FrequencyModelConfig:
    """Configuration constants for the frequency model"""
    EARLY_STOPPING_ROUNDS = 50
    N_TRIALS_DEFAULT = 100  # Increased for better hyperparameter optimization on small datasets
    RANDOM_STATE = 12
   
    # Test year (2024 reserved for testing)
    TEST_YEAR = 2024
    
    # Train/validation split parameters
    TRAIN_SIZE = 0.8  # 80% train, 20% validation from non-test data
   
    # Hyperparameter dimensions for Gaussian Process Bayesian optimization
    # Optimized for small datasets with lower eta and moderate n_estimators (750)
    PARAM_DIMENSIONS = [
        Integer(3, 8, name='max_depth'),
        Real(0.005, 0.2, prior='log-uniform', name='eta'),  # Lowered for slower, more stable learning
        Real(0.6, 1.0, name='subsample'),
        Real(0.6, 1.0, name='colsample_bytree'),
        Integer(1, 5, name='min_child_weight'),
        Real(0.0, 1.0, name='gamma'),
        Real(0.0, 2.0, name='max_delta_step')
    ]


class FrequencyModel:
   
    def __init__(self, config: Optional[FrequencyModelConfig] = None):
        self.config = config or FrequencyModelConfig()
        self.random_state = self.config.RANDOM_STATE
        self.model = None
        self.preprocessor = None  # sklearn Pipeline (ColumnTransformer + FeatureFilter)
        self.feature_names = None
        self.target_feature = "NUM_LOSSES"
        self.objective = "count:poisson"
        self.eval_metric = "poisson-nloglik"
        self.monotone_constraints = None
        self.best_params = None
        self.evaluation_results = None
        
        # Model metadata
        self.training_metadata = {}

    def load_csv_data(self, policy_file: str, account_file: str, claims_file: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """Load CSV files for frequency modeling"""
        policy_list_all = pd.read_csv(policy_file)
        account_list = pd.read_csv(account_file)
        account_list = account_list.rename(columns={col: col.upper() for col in account_list.columns})
        claims_list = pd.read_csv(claims_file)
        
        return policy_list_all, account_list, claims_list

    def validate_features(self, data: pd.DataFrame, required_features: List[str]):
        """Validate all required features are present"""
        missing_features = set(required_features) - set(data.columns)
        if missing_features:
            raise ValueError(
                f"Missing required features for model prediction: {missing_features}\n"
                f"Available features: {list(data.columns)}\n"
                f"This indicates a data pipeline issue that needs to be fixed upstream."
            )

    def create_random_splits(self, df: pd.DataFrame, year_col: str = 'WRITN_YEAR') -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        Create random train/validation splits with 2024 as test set
        """
        # Separate test data (2024)
        test_data = df[df[year_col] == self.config.TEST_YEAR].copy()
        
        # Get non-test data for train/validation split
        non_test_data = df[df[year_col] != self.config.TEST_YEAR].copy()
        
        # Random split for train/validation
        if len(non_test_data) > 0:
            # Create stratification groups for better splitting
            stratify_groups = self.create_grouped_stratification(non_test_data[self.target_feature])
            
            train_data, val_data = train_test_split(
                non_test_data,
                test_size=(1 - self.config.TRAIN_SIZE),
                random_state=self.random_state,
                stratify=stratify_groups
            )
        else:
            train_data = pd.DataFrame()
            val_data = pd.DataFrame()
        
        print(f"Random splits created:")
        print(f"  Train (random): {len(train_data):,} accounts")
        print(f"  Validation (random): {len(val_data):,} accounts")
        print(f"  Test ({self.config.TEST_YEAR}): {len(test_data):,} accounts")
        
        return train_data, val_data, test_data

    def create_grouped_stratification(self, num_losses: pd.Series) -> pd.Series:
        """Group rare values for stratification"""
        def group_losses(x):
            if pd.isna(x):
                return "missing"
            elif x == 0:
                return "zero"
            elif x == 1:
                return "one"
            elif x == 2:
                return "two"
            else:
                return "three_plus"
        
        return num_losses.apply(group_losses)

    def create_preprocessor(self, non_ohe_features: List[str], ohe_features: List[str]) -> Pipeline:
        """Create sklearn Pipeline with ColumnTransformer and FeatureFilter"""
        
        column_transformer = ColumnTransformer(
            transformers=[
                ('num', 'passthrough', [f.upper() for f in non_ohe_features]),
                ('cat', OneHotEncoder(sparse_output=False, drop=None, handle_unknown='ignore'), 
                 [f.upper() for f in ohe_features])
            ],
            remainder='drop'
        )
        
        # Create full pipeline
        pipeline = Pipeline([
            ('preprocessor', column_transformer),
            ('feature_filter', FeatureFilter())
        ])
        
        return pipeline

    def prepare_data(self, account_data: pd.DataFrame, target_feature: str) -> pd.DataFrame:
        """Prepare data with basic cleaning and type conversions"""
        # Convert column names to uppercase
        data = account_data.rename(columns={col: col.upper() for col in account_data.columns})
        target_feature = target_feature.upper()
        
        # Handle HIST_LR_3YEAR special case
        if "HIST_LR_3YEAR" in data.columns:
            data["HIST_LR_3YEAR"] = data["HIST_LR_3YEAR"].apply(
                lambda x: np.nan if pd.isna(x) else 1 if x > 1 else x
            )
        
        # Convert object columns to category for better handling
        for col in data.select_dtypes(include=['object']).columns:
            data[col] = data[col].astype('category')
        
        # Set target variable
        if target_feature in data.columns:
            data["target"] = data[target_feature]
        
        return data

    def create_monotone_constraints(self, feature_names: List[str]) -> str:
        """Create monotone constraints for frequency model"""
        constraints = []
        
        for col in feature_names:
            if "A. BELOW" in col:
                constraints.append("-1")  # Smaller band → fewer claims
            elif any(x in col for x in ["B. FROM", "C. OVER", "D. UNKNOWN", "LR_3YEAR"]):
                constraints.append("1")   # Larger band or higher LR → more claims
            else:
                constraints.append("0")   # No constraint
        
        constraint_string = f"({','.join(constraints)})"
        
        # Summary
        n_increasing = constraints.count("1")
        n_decreasing = constraints.count("-1")
        n_none = constraints.count("0")
        
        print(f"Monotone constraints: {n_increasing} increasing, {n_decreasing} decreasing, {n_none} unconstrained")
        
        return constraint_string

    def optimize_hyperparameters(self, X_train: np.ndarray, y_train: np.ndarray, 
                                X_val: np.ndarray, y_val: np.ndarray,
                                n_trials: int = 100) -> Tuple[Dict[str, Any], float]:
        """
        Gaussian Process Bayesian optimization using scikit-optimize
        
        Uses gp_minimize for sample-efficient hyperparameter optimization.
        Ideal for small-medium parameter spaces (7 parameters) with 100 trials.
        """
        
        print(f"Starting Gaussian Process Bayesian Optimization ({n_trials} trials)...")
        print("Using conservative hyperparameters optimized for small datasets:")
        print("  - Lower eta range (0.005-0.2) for gradual learning")
        print("  - n_estimators=750 with early stopping for convergence")
        
        # Define objective function using decorator
        @use_named_args(self.config.PARAM_DIMENSIONS)
        def objective(**params):
            """
            Objective function for Gaussian Process optimization
            Returns value to minimize (positive Poisson deviance)
            """
            try:
                # Create XGBRegressor with current parameters
                model = xgb.XGBRegressor(
                    objective='count:poisson',
                    booster='gbtree',
                    tree_method='exact',
                    monotone_constraints=self.monotone_constraints,
                    early_stopping_rounds=self.config.EARLY_STOPPING_ROUNDS,
                    eval_metric='poisson-nloglik',
                    random_state=self.random_state,
                    seed=self.random_state,
                    n_estimators=750,  # Manager's recommendation for small datasets with lower eta
                    validate_parameters=True,
                    **params  # Unpack the hyperparameters
                )
                
                # Fit model with early stopping
                model.fit(
                    X_train, y_train,
                    eval_set=[(X_val, y_val)],
                    eval_metric='poisson-nloglik',
                    verbose=False
                )
                
                # Make predictions and calculate Poisson deviance
                y_pred = model.predict(X_val)
                deviance = mean_poisson_deviance(y_val, y_pred)
                
                return deviance  # Return positive deviance (to minimize)
                
            except Exception as e:
                print(f"Error in objective function: {e}")
                return 999999  # Large positive value for failed trials
        
        # Gaussian Process Bayesian optimization (most sample-efficient)
        result = gp_minimize(
            func=objective,
            dimensions=self.config.PARAM_DIMENSIONS,
            n_calls=n_trials,
            n_initial_points=10,  # Random exploration phase
            random_state=self.random_state,
            verbose=True
        )
        
        # Extract best parameters
        best_params = {}
        for i, param_name in enumerate([dim.name for dim in self.config.PARAM_DIMENSIONS]):
            best_params[param_name] = result.x[i]
        
        best_score = result.fun  # This is the minimized Poisson deviance
        
        print(f"Optimization complete! Best validation score: {best_score:.6f}")
        print("Best parameters:")
        for param, value in best_params.items():
            if isinstance(value, float):
                print(f"  {param}: {value:.4f}")
            else:
                print(f"  {param}: {value}")
        
        return best_params, best_score

    def train_model(self, account_data: pd.DataFrame,
                   non_ohe_features: List[str],
                   ohe_features: List[str],
                   year_col: str = 'WRITN_YEAR',
                   n_trials: int = None) -> Dict[str, Any]:
        """
        Train frequency model with sklearn pipeline preprocessing and GP optimization
        """
        
        n_trials = n_trials or self.config.N_TRIALS_DEFAULT
        start_time = datetime.now()
        print("="*60)
        print("STARTING FREQUENCY MODEL TRAINING")
        print("="*60)
        
        # Store training metadata
        self.training_metadata = {
            'start_time': start_time.isoformat(),
            'n_trials': n_trials,
            'optimization_method': 'Gaussian Process (gp_minimize)',
            'features': {
                'non_ohe': non_ohe_features,
                'ohe': ohe_features
            }
        }
        
        # 1. Create random splits
        print("Step 1: Creating random splits...")
        train_data, val_data, test_data = self.create_random_splits(account_data, year_col)
        
        # 2. Prepare data
        target_col = self.target_feature if self.target_feature in account_data.columns else self.target_feature.lower()
        
        print("Step 2: Preparing data...")
        train_prepared = self.prepare_data(train_data, target_col)
        val_prepared = self.prepare_data(val_data, target_col)
        test_prepared = self.prepare_data(test_data, target_col)
        
        # 3. Create and fit preprocessor pipeline
        print("Step 3: Creating and fitting preprocessing pipeline...")
        self.preprocessor = self.create_preprocessor(non_ohe_features, ohe_features)
        
        # Get feature columns (exclude target) and validate they exist
        all_features = [f.upper() for f in non_ohe_features + ohe_features]
        
        # Validate that all required features are present in training data
        self.validate_features(train_prepared, all_features)
        print(f"✅ All {len(all_features)} required features validated in training data")
        
        # Validate that all required features are present in validation data  
        self.validate_features(val_prepared, all_features)
        print(f"✅ All {len(all_features)} required features validated in validation data")
        
        # Now we can safely use the features
        available_features = all_features  # All are validated to be present
        
        # Fit pipeline on training data
        X_train = train_prepared[available_features]
        y_train = train_prepared["target"]
        
        # Fit and transform training data using the pipeline
        X_train_processed = self.preprocessor.fit_transform(X_train)
        
        # Transform validation data using the fitted pipeline
        X_val = val_prepared[available_features]
        y_val = val_prepared["target"]
        X_val_processed = self.preprocessor.transform(X_val)
        
        # Get feature names from the pipeline
        # First get names from the column transformer
        column_transformer = self.preprocessor.named_steps['preprocessor']
        raw_feature_names = column_transformer.get_feature_names_out()
        
        # Then get the filtered feature names
        feature_filter = self.preprocessor.named_steps['feature_filter']
        self.feature_names = feature_filter.get_feature_names_out(raw_feature_names)
        
        print(f"Features after column transformation: {len(raw_feature_names)}")
        print(f"Features after filtering: {len(self.feature_names)}")
        print(f"Training data: {X_train_processed.shape}")
        print(f"Validation data: {X_val_processed.shape}")
        
        # 4. Create monotone constraints
        print("Step 4: Creating monotone constraints...")
        self.monotone_constraints = self.create_monotone_constraints(self.feature_names)
        
        # 5. Gaussian Process Bayesian optimization
        print("Step 5: Hyperparameter optimization using Gaussian Process...")
        self.best_params, best_score = self.optimize_hyperparameters(
            X_train_processed, y_train.values, 
            X_val_processed, y_val.values, 
            n_trials
        )
        
        # 6. Train final model with best parameters
        print("Step 6: Training final model...")
        
        # Create final XGBRegressor with best parameters
        final_params = {
            "objective": self.objective,
            "eval_metric": self.eval_metric,
            "booster": "gbtree",
            "tree_method": "exact",
            "monotone_constraints": self.monotone_constraints,
            "early_stopping_rounds": self.config.EARLY_STOPPING_ROUNDS,
            "validate_parameters": True,
            "random_state": self.random_state,
            "seed": self.random_state,
            "n_estimators": 750  # Manager's recommendation for small datasets with lower eta
        }
        final_params.update(self.best_params)
        
        self.model = xgb.XGBRegressor(**final_params)
        
        print("Training final frequency model with best parameters...")
        print(f"Using lower eta range (0.005-0.2) and n_estimators=750 for stable learning on small dataset")
        
        # Train with validation set for early stopping
        self.model.fit(
            X_train_processed, y_train,
            eval_set=[(X_train_processed, y_train), (X_val_processed, y_val)],
            eval_metric='poisson-nloglik',
            verbose=50  # Print every 50 rounds
        )
        
        # Get best iteration and score from the model
        best_iteration = self.model.best_iteration if hasattr(self.model, 'best_iteration') else len(self.model.get_booster().get_dump())
        
        print(f"Final model: {best_iteration} trees, best score: {best_score:.6f}")
        
        # 7. Evaluate on test data
        print("Step 7: Evaluating on test data...")
        
        # Validate test data has required features
        self.validate_features(test_prepared, available_features)
        print(f"✅ All {len(available_features)} required features validated in test data")
        
        test_predictions = self.predict(test_prepared, available_features)
        test_actual = test_prepared["target"].values
        
        self.evaluation_results = self.comprehensive_evaluation(
            test_prepared, test_actual, test_predictions
        )
        
        # Complete training metadata
        end_time = datetime.now()
        self.training_metadata.update({
            'end_time': end_time.isoformat(),
            'training_duration': str(end_time - start_time),
            'best_iteration': best_iteration,
            'best_score': best_score,
            'xgboost_version': xgb.__version__,
            'total_features': len(self.feature_names)
        })
        
        print(f"\nTraining completed successfully in {end_time - start_time}")
        print("="*60)
        
        return {
            "model": self.model,
            "best_params": self.best_params,
            "best_score": best_score,
            "feature_names": self.feature_names,
            "preprocessor": self.preprocessor,
            "evaluation_results": self.evaluation_results,
            "train_data": train_data,
            "val_data": val_data,
            "test_data": test_data,
            "training_metadata": self.training_metadata
        }

    def predict(self, new_data: pd.DataFrame, feature_columns: List[str]) -> np.ndarray:
        """Make predictions using the fitted preprocessing pipeline and model"""
        if self.model is None:
            raise ValueError("Model not trained yet. Call train_model() first.")
        
        if self.preprocessor is None:
            raise ValueError("Preprocessing pipeline not fitted. Model may not be properly trained.")
        
        # Validate that all required features are present
        required_features = [f.upper() for f in feature_columns]
        self.validate_features(new_data, required_features)
        print(f"✅ All {len(required_features)} required features validated in prediction data")
        
        # Use only the specified feature columns (all validated to be present)
        X = new_data[required_features]
        
        # Apply the full preprocessing pipeline (ColumnTransformer + FeatureFilter)
        X_processed = self.preprocessor.transform(X)
        
        print(f"Prediction data processed through pipeline: {X_processed.shape}")
        
        # Make predictions using sklearn XGBRegressor
        return self.model.predict(X_processed)

    def comprehensive_evaluation(self, test_data: pd.DataFrame, y_test: np.ndarray, predictions: np.ndarray) -> Dict[str, Any]:
        """Comprehensive evaluation metrics for frequency model"""
        
        # Basic Performance Metrics using sklearn's mean_poisson_deviance
        basic_metrics = {
            'poisson_deviance': mean_poisson_deviance(y_test, predictions),
            'mae': mean_absolute_error(y_test, predictions),
            'rmse': np.sqrt(mean_squared_error(y_test, predictions)),
            'mean_actual': y_test.mean(),
            'mean_predicted': predictions.mean(),
            'mean_ratio': predictions.mean() / y_test.mean() if y_test.mean() > 0 else np.nan,
            'n_test_accounts': len(y_test)
        }
        
        # RMSE/NRMSE analysis
        rmse_value = basic_metrics['rmse']
        nrmse_results = {
            "sd": rmse_value / np.std(y_test),
            "mean": rmse_value / np.mean(y_test),
            "median": rmse_value / np.median(y_test),
            "maxmin": rmse_value / (np.max(y_test) - np.min(y_test))
        }
        
        # Distribution comparison
        actual_dist = pd.Series(y_test).value_counts().sort_index()
        pred_dist = pd.Series(np.round(predictions)).value_counts().sort_index()
        
        all_values = sorted(set(list(actual_dist.index) + list(pred_dist.index)))
        actual_dist = actual_dist.reindex(all_values, fill_value=0)
        pred_dist = pred_dist.reindex(all_values, fill_value=0)
        
        distribution_comparison = pd.DataFrame({
            'Actual_Count': actual_dist,
            'Predicted_Count': pred_dist,
            'Actual_Pct': actual_dist / len(y_test) * 100,
            'Predicted_Pct': pred_dist / len(predictions) * 100
        })
        
        # Yearly analysis if year column exists
        yearly_analysis = None
        if 'WRITN_YEAR' in test_data.columns:
            test_results = test_data.copy()
            test_results['predicted_losses'] = predictions
            test_results['actual_losses'] = y_test
            
            yearly_analysis = test_results.groupby('WRITN_YEAR').agg({
                'actual_losses': ['count', 'mean', 'sum'],
                'predicted_losses': ['mean', 'sum']
            }).round(4)
        
        return {
            'basic_metrics': basic_metrics,
            'nrmse_results': nrmse_results,
            'distribution_comparison': distribution_comparison,
            'yearly_analysis': yearly_analysis
        }

    def get_feature_importance(self, importance_type: str = 'weight') -> pd.DataFrame:
        """Get feature importance as a DataFrame"""
        if self.model is None:
            raise ValueError("Model not trained yet.")
        
        # For sklearn XGBRegressor, get importance from the booster
        if hasattr(self.model, 'get_booster'):
            importance = self.model.get_booster().get_score(importance_type=importance_type)
        else:
            # Fallback to feature_importances_ attribute
            importance_values = getattr(self.model, 'feature_importances_', None)
            if importance_values is None:
                raise ValueError("No feature importance available from model")
            importance = dict(zip(self.feature_names, importance_values))
        
        # Convert to DataFrame
        importance_df = pd.DataFrame([
            {'feature': k, 'importance': v}
            for k, v in importance.items()
        ])
        
        # Sort by importance
        importance_df = importance_df.sort_values('importance', ascending=False)
        importance_df['rank'] = range(1, len(importance_df) + 1)
        importance_df['importance_pct'] = (importance_df['importance'] /
                                          importance_df['importance'].sum() * 100)
        
        return importance_df

    def save_model(self, filepath: str):
        """Save the complete trained frequency model and sklearn pipeline"""
        if self.model is None:
            raise ValueError("Model not trained yet.")
        
        model_artifacts = {
            "model": self.model,
            "preprocessor": self.preprocessor,
            "feature_names": self.feature_names,
            "target_feature": self.target_feature,
            "objective": self.objective,
            "eval_metric": self.eval_metric,
            "monotone_constraints": self.monotone_constraints,
            "best_params": self.best_params,
            "evaluation_results": self.evaluation_results,
            "training_metadata": self.training_metadata,
            "config": self.config
        }
        
        joblib.dump(model_artifacts, filepath)
        print(f"Frequency model saved to {filepath}")

    def load_model(self, filepath: str):
        """Load a trained frequency model and sklearn pipeline"""
        model_artifacts = joblib.load(filepath)
        
        self.model = model_artifacts["model"]
        self.preprocessor = model_artifacts["preprocessor"]
        self.feature_names = model_artifacts["feature_names"]
        self.target_feature = model_artifacts["target_feature"]
        self.objective = model_artifacts["objective"]
        self.eval_metric = model_artifacts["eval_metric"]
        self.monotone_constraints = model_artifacts["monotone_constraints"]
        self.best_params = model_artifacts["best_params"]
        self.evaluation_results = model_artifacts["evaluation_results"]
        self.training_metadata = model_artifacts.get("training_metadata", {})
        self.config = model_artifacts.get("config", FrequencyModelConfig())
        
        print(f"Frequency model loaded from {filepath}")

    def plot_feature_importance(self, max_features: int = 20):
        """Plot feature importance"""
        if self.model is None:
            raise ValueError("Model not trained yet.")
        
        fig, ax = plt.subplots(figsize=(10, 8))
        
        # For sklearn XGBRegressor, plot using the booster
        if hasattr(self.model, 'get_booster'):
            xgb.plot_importance(self.model.get_booster(), importance_type='weight',
                               max_num_features=max_features, ax=ax)
        else:
            # Fallback: create manual plot from feature_importances_
            importance_df = self.get_feature_importance()
            top_features = importance_df.head(max_features)
            ax.barh(range(len(top_features)), top_features['importance'])
            ax.set_yticks(range(len(top_features)))
            ax.set_yticklabels(top_features['feature'])
            ax.set_xlabel('Importance')
            
        plt.title("Frequency Model - Feature Importance")
        plt.tight_layout()
        return fig

    def print_evaluation_summary(self):
        """Print comprehensive evaluation summary"""
        if self.evaluation_results is None:
            print("No evaluation results available. Train model first.")
            return
        
        print("="*60)
        print("FREQUENCY MODEL EVALUATION SUMMARY")
        print("="*60)
        
        print("\n1. BASIC PERFORMANCE METRICS:")
        for metric, value in self.evaluation_results['basic_metrics'].items():
            if metric != 'n_test_accounts':
                print(f"   {metric}: {value:.6f}")
            else:
                print(f"   {metric}: {value:,}")
        
        print("\n2. NORMALIZED RMSE:")
        for norm_type, value in self.evaluation_results['nrmse_results'].items():
            print(f"   NRMSE ({norm_type}): {value:.6f}")
        
        print("\n3. DISTRIBUTION COMPARISON:")
        print(self.evaluation_results['distribution_comparison'])
        
        if self.evaluation_results['yearly_analysis'] is not None:
            print("\n4. YEARLY ANALYSIS:")
            print(self.evaluation_results['yearly_analysis'])


# Helper function for quick model training
def train_frequency_model(account_data: pd.DataFrame,
                         non_ohe_features: List[str],
                         ohe_features: List[str],
                         n_trials: int = 100,
                         save_path: Optional[str] = None) -> FrequencyModel:
    """
    Convenience function to train a frequency model using Gaussian Process optimization
    
    Uses gp_minimize for optimal hyperparameter search on small-medium parameter spaces.
    """
    # Initialize model
    freq_model = FrequencyModel()
    
    # Train using Gaussian Process optimization
    results = freq_model.train_model(
        account_data,
        non_ohe_features,
        ohe_features,
        n_trials=n_trials
    )
    
    # Print summary
    freq_model.print_evaluation_summary()
    
    # Save if path provided
    if save_path:
        freq_model.save_model(save_path)
        print(f"\nModel saved to: {save_path}")
    
    return freq_model
